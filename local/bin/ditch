#!/bin/sh --
# ditch - create a menu using the twitch team and stream APIs

# requires awk jq curl bemenu mpv youtube-dl

# LOCALDIR/data/ditch/streamers  list of individual streamers

# If the streamers file doesn't exist then fall back on stdin.

readonly argv0=ditch
readonly TMPDIR=${TMPDIR:-/tmp}
data=${LOCALDIR:-$HOME/local}/data/$argv0/streamers

# Number of entries to display in the menu.
readonly lines=30

# I don't care about making these API keys public, use it if you want.
# Backup: jzkbprff40iqj646a697cyrvl0zt2m6 (youtube-dl)
readonly client_id=rfbp97unyyikyzw1y9cy87iii29lgjk

# Make sure we remove the json once we're done so it doesn't interfere with
# proceeding execution and doesn't clutter up $TMPDIR.
trap 'rm -f -- "$TMPDIR/$argv0"-????' EXIT

chunkurls() {
    # Twitch allows up to a 100 streamers in a single query when the limit is
    # raised using &limit=100.  This awk script chops up the list of streamers
    # into chunks of 100, each comma separated and appends the &limit= with an
    # appropriate value.
    awk -v interval=100 '
    BEGIN{
        i = 1
    }

    NF < 1 {
        noinput=1
        exit
    }

    i == 1 {
        printf "https://api.twitch.tv/kraken/streams?channel="
    }

    (NR % interval){
        ORS=","
        print $0
        i++
    }

    !(NR % interval){
        ORS="\n"
        print $0 "&limit=" i
        i = 1
    }

    END{
        if(noinput)
            exit

        ORS="\n"
        print "&limit=" i
    }'
}

readjson() {
    # Using '│' as a marker here for column to pretty print the results in
    # columns, assuming not many people use this particular sentinel in their
    # titles.  From experience this hasn't been the case.  In either case it
    # won't break functionality as only the first word is needed which itself
    # is guaranteed to have no spaces or high unicode plane codepoints.
    jq -r '.streams[] |
          "\(.channel.name)│\(.channel.game)│(\(.viewers)) \(.channel.status)"'
}

# NOTE: This leaves the responsibility of multi-tasking potentially many
#       processes to the kernel.  Neither gnu parallel(1) or xargs(1) proved to
#       be very capable of this without complication and awkwardness, and
#       neither were any faster.
pulljson() {
    local tmp url

    while read -r -- url; do
        if [ "$url" ]; then
            tmp=$(mktemp -t "$argv0"-XXXX)
            curl -sH "Client-ID: $client_id" "$url" | readjson > "$tmp" &
        fi
    done

    wait
}

stream() {
    local streamer

    while read -r streamer _; do
        if [ "$streamer" ]; then
            # Background mpv here in order to support multi-selection.
            mpv --quiet https://twitch.tv/"$streamer" &
        fi
    done
}

if [ ! -s "$data" ] || [ "$1" = '-' ]; then
    data=/dev/stdin
fi

# By splitting the pipeline in two we can create all our pre-formatted files in
# parallel in one half and do all the rendering and selection handling in the
# second without complication.
chunkurls < "$data" | pulljson

# Sort based on viewer count.
column -s '│' -t "$TMPDIR/$argv0"-???? | sort -srnk 2,2 -t '(' |\
    menu --ifne -l "$lines" -p 'STREAM' | stream
exit
